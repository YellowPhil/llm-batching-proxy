services:
  text-embeddings-inference:
    image: ghcr.io/huggingface/text-embeddings-inference:cpu-ipex-latest
    command: --model-id Qwen/Qwen3-Embedding-0.6B
    networks:
      - rust-service-network
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "http://localhost:80/health"]
      interval: 30s
      timeout: 10s

  auto-batching-proxy:
    build: .
    ports:
      - "3000:3000"
    networks:
      - rust-service-network
    depends_on:
      text-embeddings-inference:
        condition: service_healthy
    volumes:
      - ./config.toml:/app/config.toml:ro
    command: ["--config", "/app/config.toml", "--port", "3000", "--inference-url", "http://text-embeddings-inference:80/embed"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s

networks:
  rust-service-network:
    driver: bridge